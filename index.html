<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Azure OpenAI Realtime Session</title>
    <!-- Include Azure Speech SDK -->
    <script src="https://aka.ms/csspeech/jsbrowserpackageraw"></script>
    <style>
      body {
        font-family: Arial, sans-serif;
        margin: 20px;
        line-height: 1.6;
      }
      #logContainer {
        margin-top: 20px;
        padding: 10px;
        border: 1px solid #ccc;
        max-height: 300px;
        overflow-y: auto;
        background-color: #f9f9f9;
      }
      button {
        padding: 8px 15px;
        margin: 5px;
        cursor: pointer;
      }
      .listening {
        color: green;
        font-weight: bold;
      }
      .not-listening {
        color: red;
      }
      .closing {
        color: orange;
        font-weight: bold;
        animation: blink 1s infinite;
      }
      @keyframes blink {
        0% {
          opacity: 1;
        }
        50% {
          opacity: 0.5;
        }
        100% {
          opacity: 1;
        }
      }
      #voiceStatus {
        margin: 10px 0;
      }
    </style>
  </head>
  <body>
    <h1>Azure OpenAI Realtime Session</h1>
    <p>
      WARNING: Don't use this code sample in production with the API key
      hardcoded. Use a protected backend service to call the sessions API and
      generate the ephemeral key. Then return the ephemeral key to the client.
    </p>
    <button id="startSessionBtn" onclick="StartSession()">Start Session</button>
    <p id="voiceStatus" class="listening">
      Voice trigger: Always listening for "assistant" or "Hey assistant"
    </p>
    <p>
      <small
        >During an active session, say <strong>"stop"</strong> or
        <strong>"stop it shut up"</strong> to close the session.</small
      >
    </p>

    <!-- Log container for API messages -->
    <div id="logContainer"></div>

    <script>
      // Make sure the WebRTC URL region matches the region of your Azure OpenAI resource.
      // For example, if your Azure OpenAI resource is in the swedencentral region,
      // the WebRTC URL should be https://swedencentral.realtimeapi-preview.ai.azure.com/v1/realtimertc.
      // If your Azure OpenAI resource is in the eastus2 region, the WebRTC URL should be https://eastus2.realtimeapi-preview.ai.azure.com/v1/realtimertc.
      const WEBRTC_URL =
        "https://eastus2.realtimeapi-preview.ai.azure.com/v1/realtimertc";

      // The SESSIONS_URL includes the Azure OpenAI resource URL,
      // deployment name, the /realtime/sessions path, and the API version.
      // The Azure OpenAI resource region isn't part of the SESSIONS_URL.
      const SESSIONS_URL =
        "https://o1-test-pgaddam.openai.azure.com/openai/realtimeapi/sessions?api-version=2025-04-01-preview";

      // The API key of the Azure OpenAI resource.
      const API_KEY = "";

      // The deployment name might not be the same as the model name.
      const DEPLOYMENT = "gpt-4o-mini-realtime-preview";
      const VOICE = "verse";

      // Speech recognition variables
      let recognition = null;
      let isListening = false;

      // Variables to track WebRTC connection components
      let currentDataChannel = null;
      let currentPeerConnection = null;

      // Available language fallbacks in order of preference
      const languageFallbacks = ["en-US", "en", "en-GB", "en-AU"];
      let currentLanguageIndex = 0;

      // Initialize speech recognition
      function initSpeechRecognition() {
        if (
          !("webkitSpeechRecognition" in window) &&
          !("SpeechRecognition" in window)
        ) {
          logMessage("Speech recognition not supported in this browser.");
          return false;
        }

        // Create a recognition instance
        recognition = new (window.SpeechRecognition ||
          window.webkitSpeechRecognition)();

        // Configure recognition
        recognition.continuous = true;
        recognition.interimResults = false;

        // Try to use the user's browser language first, then fall back to our list
        try {
          // Start with browser language as first choice
          const browserLanguage = navigator.language || languageFallbacks[0];
          recognition.lang = browserLanguage;
          logMessage(`Using browser language: ${browserLanguage}`);

          // Add browser language to beginning of fallback list if not already there
          if (!languageFallbacks.includes(browserLanguage)) {
            languageFallbacks.unshift(browserLanguage);
          }
        } catch (e) {
          recognition.lang = languageFallbacks[0];
          logMessage(`Fallback to ${languageFallbacks[0]} language`);
        }

        // Handle recognition results
        recognition.onresult = function (event) {
          const transcript = event.results[
            event.results.length - 1
          ][0].transcript
            .trim()
            .toLowerCase();
          logMessage("You said: " + transcript);

          // Check for trigger phrases
          if (transcript === "assistant" || transcript === "hey assistant") {
            logMessage("Trigger phrase detected! Starting session...");
            document.getElementById("startSessionBtn").click();
          }

          // Check for stop phrases
          if (
            transcript === "stop" ||
            transcript.includes("stop it") ||
            transcript.includes("shut up")
          ) {
            logMessage("Stop phrase detected! Closing session...");
            if (currentDataChannel || currentPeerConnection) {
              stopSession();
            } else {
              logMessage("No active session to close.");
            }
          }
        };

        recognition.onerror = function (event) {
          logMessage("Speech recognition error: " + event.error);

          // Handle specific errors
          if (event.error === "language-not-supported") {
            // Try the next language in our fallback list
            currentLanguageIndex++;
            if (currentLanguageIndex < languageFallbacks.length) {
              const nextLang = languageFallbacks[currentLanguageIndex];
              logMessage(
                `Language not supported. Trying next language: ${nextLang}`
              );
              recognition.lang = nextLang;
            } else {
              // We've tried all languages, reset to first one
              currentLanguageIndex = 0;
              logMessage(
                "Tried all language options. Restarting with " +
                  languageFallbacks[0]
              );
              recognition.lang = languageFallbacks[0];
            }

            setTimeout(() => {
              try {
                recognition.start();
                logMessage(
                  `Restarting speech recognition with ${recognition.lang}...`
                );
              } catch (e) {
                logMessage("Error restarting speech recognition: " + e.message);
              }
            }, 1000);
          } else {
            // Handle other errors
            setTimeout(() => {
              try {
                recognition.start();
                logMessage("Restarting speech recognition after error...");
              } catch (e) {
                logMessage("Error restarting speech recognition: " + e.message);
              }
            }, 1000);
          }
        };

        recognition.onend = function () {
          // Always restart when it ends
          try {
            recognition.start();
            logMessage("Restarting speech recognition...");
          } catch (e) {
            logMessage("Error restarting speech recognition: " + e.message);
            // Try again after a delay
            setTimeout(() => {
              try {
                recognition.start();
              } catch (e2) {
                logMessage("Failed to restart speech recognition.");
              }
            }, 1000);
          }
        };

        // Start listening immediately
        recognition.start();
        isListening = true;
        logMessage(
          "Voice recognition started - say 'assistant' or 'Hey assistant' to start session"
        );

        return true;
      }

      // Start speech recognition when the page loads
      document.addEventListener("DOMContentLoaded", function () {
        if (initSpeechRecognition()) {
          logMessage(
            "Voice recognition is active. Say 'assistant' or 'Hey assistant' to start a session."
          );
        } else {
          logMessage(
            "Could not initialize speech recognition. Please try a different browser like Chrome or Edge."
          );
        }
      });

      async function StartSession() {
        try {
          // WARNING: Don't use this code sample in production
          // with the API key hardcoded.
          // Use a protected backend service to call the
          // sessions API and generate the ephemeral key.
          // Then return the ephemeral key to the client.

          const response = await fetch(SESSIONS_URL, {
            method: "POST",
            headers: {
              // The Authorization header is commented out because
              // currently it isn't supported with the sessions API.
              //"Authorization": `Bearer ${ACCESS_TOKEN}`,
              "api-key": API_KEY,
              "Content-Type": "application/json",
            },
            body: JSON.stringify({
              model: DEPLOYMENT,
              voice: VOICE,
            }),
          });

          if (!response.ok) {
            throw new Error(`API request failed`);
          }

          const data = await response.json();

          const sessionId = data.id;
          const ephemeralKey = data.client_secret?.value;
          console.error("Ephemeral key:", ephemeralKey);

          // Mask the ephemeral key in the log message.
          logMessage("Ephemeral Key Received: " + "***");
          logMessage("WebRTC Session Id = " + sessionId);

          // Set up the WebRTC connection using the ephemeral key.
          init(ephemeralKey);
        } catch (error) {
          console.error("Error fetching ephemeral key:", error);
          logMessage("Error fetching ephemeral key: " + error.message);
        }
      }

      async function init(ephemeralKey) {
        let peerConnection = new RTCPeerConnection();
        // Store the peerConnection in the global variable
        currentPeerConnection = peerConnection;

        // Set up to play remote audio from the model.
        const audioElement = document.createElement("audio");
        audioElement.autoplay = true;
        document.body.appendChild(audioElement);

        peerConnection.ontrack = (event) => {
          audioElement.srcObject = event.streams[0];
        };

        // Set up data channel for sending and receiving events
        const clientMedia = await navigator.mediaDevices.getUserMedia({
          audio: true,
        });
        const audioTrack = clientMedia.getAudioTracks()[0];
        peerConnection.addTrack(audioTrack);

        const dataChannel =
          peerConnection.createDataChannel("realtime-channel");
        // Store the dataChannel in the global variable
        currentDataChannel = dataChannel;

        dataChannel.addEventListener("open", () => {
          logMessage("Data channel is open");
          updateSession(dataChannel);
        });

        dataChannel.addEventListener("message", (event) => {
          const realtimeEvent = JSON.parse(event.data);
          console.log(realtimeEvent);
          logMessage(
            "Received server event: " + JSON.stringify(realtimeEvent, null, 2)
          );
          if (realtimeEvent.type === "session.update") {
            const instructions = realtimeEvent.session.instructions;
            logMessage("Instructions: " + instructions);
          } else if (realtimeEvent.type === "session.error") {
            logMessage("Error: " + realtimeEvent.error.message);
          } else if (realtimeEvent.type === "session.end") {
            logMessage("Session ended.");
          }
        });

        dataChannel.addEventListener("close", () => {
          logMessage("Data channel is closed");
        });

        // Start the session using the Session Description Protocol (SDP)
        const offer = await peerConnection.createOffer();
        await peerConnection.setLocalDescription(offer);

        const sdpResponse = await fetch(`${WEBRTC_URL}?model=${DEPLOYMENT}`, {
          method: "POST",
          body: offer.sdp,
          headers: {
            Authorization: `Bearer ${ephemeralKey}`,
            "Content-Type": "application/sdp",
          },
        });

        const answer = { type: "answer", sdp: await sdpResponse.text() };
        await peerConnection.setRemoteDescription(answer);

        const button = document.createElement("button");
        button.innerText = "Close Session";
        button.onclick = stopSession;
        document.body.appendChild(button);

        // Send a client event to update the session
        function updateSession(dataChannel) {
          const event = {
            type: "session.update",
            session: {
              instructions:
                "You are a helpful AI assistant responding in natural, engaging language.",
            },
          };
          dataChannel.send(JSON.stringify(event));
          logMessage("Sent client event: " + JSON.stringify(event, null, 2));
        }
      }

      function stopSession() {
        // Update voice status to show closing
        const voiceStatus = document.getElementById("voiceStatus");
        voiceStatus.textContent = "Closing session...";
        voiceStatus.className = "closing";

        // After a short delay, close the session
        setTimeout(() => {
          if (currentDataChannel) {
            currentDataChannel.close();
            currentDataChannel = null;
          }
          if (currentPeerConnection) {
            currentPeerConnection.close();
            currentPeerConnection = null;
          }
          logMessage("Session closed.");

          // Reset voice status
          voiceStatus.textContent =
            'Voice trigger: Always listening for "assistant" or "Hey assistant"';
          voiceStatus.className = "listening";

          // Remove the close session button if it exists
          const closeButtons = document.querySelectorAll("button");
          closeButtons.forEach((button) => {
            if (button.innerText === "Close Session") {
              button.remove();
            }
          });
        }, 1000);
      }

      function logMessage(message) {
        const logContainer = document.getElementById("logContainer");
        const p = document.createElement("p");
        p.textContent = message;
        logContainer.appendChild(p);
      }
    </script>
  </body>
</html>
